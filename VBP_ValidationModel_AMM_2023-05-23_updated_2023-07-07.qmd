---
title: "NCQA My HEDIS 2023 Validation Model"
subtitle: "Antidepressant Medication Management"
author: 
  name: "John Ryan Kivela, MA"
  email: "Ryan.Kivela@narbha.org"
  affiliation: "The Alliance ACO"
date: today
date-format: long
format:
  html:
    theme: pulse
    embed-resources: true
    toc: true
    toc-depth: 6
    code-fold: true
    footnotes-hover: true
---

```{r}
#| label: Setup
#| eval: true
#| include: false
#| echo: false
#| warning: false
#| error: false

## Load Libraries
library(tidyverse)
library(readxl)
library(kableExtra)
library(gt)
library(scales)
library(utils)
library(lubridate)

# Create table for inline code
InLineCode <- data.frame(
  ReportDate = "04-27-2023",
  MeasurementYear = "01-01-2023 to 12-31-2023",
  ClaimsAdjudicatedThrough = "02-28-2023",
  HEDISVersion = "My 2023",
  IntakePeriod = "05-01-2022 to 04-30-2023",
  NegativeMedicationHistory = "01-16-222 to 01-15-2023",
    IPSDstart_date = as.Date("2022-05-01"),
    IPSDend_date = as.Date("2023-04-30"),
    NMHstart_date = as.Date("2022-01-16"),
    NMHend_date = as.Date("2023-01-15")
)

# Define custom inline code hook
comma_separator_hook <- function(x) {
  formatted <- format(x, big.mark = ",")
  knitr::asis_output(formatted)
}

# Set the inline code hook globally
knitr::knit_hooks$set(inline = comma_separator_hook)
```

```{r}
#| label: Demo Data
#| eval: true 
#| include: false
#| echo: false
#| warning: false
#| error: false

# This code manually loads the referenced datasets that are created below. This allows you to rework the document without running the time and computer resource intensive analyses themselves. This code can only be run if the full project below has already been completed, which it has. 

PBMClaims <- read.csv("./data/DataRaw_PBMClaims_2023-05-31.csv")

PBMClaims <- read.csv("./data/output/PBMClaims.csv")

PBMClaims_NMHTest <- ("./data/output/PBMClaims_NMHTest_copy.csv")

ValueSetListMy2023 <- read.csv("./data/ValueSetListMy2023.csv")

MDDxClaims <- read.csv("./data/MDDxClaims_20230602.csv")

# PBMClaims_MDDxTest <- read.csv("./data/output/PBMClaims_MDDxTest.csv")

GlobalMembers_orig <- read_xlsx("./data/data_original_glblmbrs_2023-05-01_globalMembersRoster.xlsx", sheet = "Sheet1")

PBMClaims_CE_Test <- read.csv("./data/output/PBMClaims_CE_Test.csv")

AMMEligibilityTest <- read.csv("./data/output/AMMEligibilityTest.csv")

DataRaw_VBPQR_AllAPsCombined <- read.csv("./data/output/2023-05-31_DataRaw_VBPQR_AllAPsCombined.csv")

VBPQR_AllAPsCombined_Cleaned2 <- read.csv("./data/output/VBPQualityRoster.csv")

VBPQR_AllAPsCombined_Cleaned <- read.csv("./data/output/VBP_Validation.csv")

VBP_Unduplicated <- read.csv("./data/output/VBP_Unduplicated.csv")

AMMClaims_Unduplicated <- ("./data/output/AMMClaims_Unduplicated.csv")

Validation_Matrix <- read.csv("./data/output/ValidationMatrix.csv")

Compliance <- read.csv("./data/output/Compliance.csv")

Comp_ChiSq <- read.csv("./data/output/Comp_ChiSq.csv")

MemberFollowUpList <- read.csv("./data/output/AMM_MemberFollowUpList.csv")
```

# Welcome *(Bien Venido)*

Hello,

This project compares Report Data with Real Data by comparing the Health Choice Value Based Purchasing Quality Roster (VBPQR)[^1] for Antidepressant Medication Management (AMM) with Health Choice adjudicated claims. This project is similar in many ways to the VBP Validation Model for Follow Up after Hospitalization (FUH)[^2].

The guidelines for antidepressant medication are far more nuanced[^3]. Nonetheless we have endeavored to define the eligible population using adjudicated claims, measuring our Real Data against the VBP Quality Report Data.

The results were surprising! Our research into eligible adjudicated claims identified a similar number of cases to the VBP Quality Roster; however, only 7% of the VBP QR members were validated against adjudicated claims. 

We hope you enjoy reading this data story. We are always looking for new collaborators, so please reach out and share your ideas.

Sincerely,

Ryan

[^1]: Health Choice Arizona. (2023). *Value Based Purchasing Quality Roster*

[^2]: Kivela, J.R. (2023). Value Based Purchasing Data Validation Model. *The Northern Arizona Regional Behavioral Health Alliance*.

[^3]: NCQA. (2023). *HEDIS and Performance Measurement*. https://www.ncqa.org/hedis/

![The Alliance NCQA HEDIS My2023 Eligibility Model for AMM](./images/AMMEligibilityModel_transparentbackground.png)

## The thing about measuring VBP

So, there we were in the first months of the Alliance ACO business, and EVERY provider was concerned that the Value Based Purchasing Quality Roster was inaccurate. Our providers were concerned that their performance scores were underrated, not giving them the credit they deserved.

The Alliance wants its providers to be their best selves and to be recognized. Lack of confidence in the scoring mechanism crowds out enthusiasm and damages commitment to improvement.

So, in January 2023 we developed a validation model for the NCQA measure for Follow Up after Hospitalization (FUH).

::: callout-tip

## The VBP Validation Model FUH7 (2023), Key Takeaways:

-   Created a validated data model that can be used to identify eligible member events for FUH7
-   Core logic can be used to develop models for other HEDIS/NCQA measures
-   Research affirmed providers were underrated for the 2022 measurement year
-   Providers deserved a higher score by about 4 percentage points
:::

### So what's next?

That brings us to the new measurement year, January 2023 to December 2023. In this project the VBP Validation Model is going to be applied to the Antidepressant Medication Management (AMM) HEDIS My2023[^4] performance measure.

This model compares **Report Data** from the VBP Quality Roster with **Real Data** from adjudicated claims.

The model will answer the following questions:

-   Do the results of the Value Based Purchasing Quality Roster (VBPQR) accurately reflect actual adjudicated claims?
-   Are the performance scores of Alliance Providers accurate and reliable?

The business objectives are twofold:

-   Build an eligibility model for the AMM eligible population
-   Validate the VBP Quality Report using adjudicated claims for eligible cases

[^4]: NCQA. (2023). *HEDIS Measures and Technical Resources*. https://www.ncqa.org/hedis/measures/

## Why invest in this research?

Inaccurate measurement of VBP performance leads to an invalid assessment of the Alliance Providers' delivery of services to their patients.

Alliance providers were underrated on Follow Up after Hospitalization (FUH7) by at least 4 percentage points for the 2022 measurement year[^5]. With the constantly increasing emphasis on payment for quality of care, even small differences in measurement impact overall performance ratings.

![Alliance Provider performance scores were underated (red blocks) by at least 4 percentage points in 2022](./images/Fig_sup_Compliance.png)

Underrating provider performance frustrates leadership and dampens the spirits of the clinical teams as they do this very difficult work.

Accurate measurement offers a better opportunity to identify patients in need, maximize performance incentive, and support provider morale.

[^5]: Kivela, J.R. (2023). Value Based Purchasing Data Validation Model. *The Northern Arizona Regional Behavioral Health Alliance*.

# Setting up the experiment

This manuscript is written using Brisk-DM. Brisk-DM is a structure for doing data science that is based on CRISP-DM[^6], but tailored for executives and business leads. Brisk-DM uses an approachable communication style that is easily understandable to professionals in the workplace.

[^6]: CRISP-DM help overview. (August 17, 2021). https://www.ibm.com/docs/en/spss-modeler/saas?topic=dm-crisp-help-overview.

## The current state of things

At the time of this writing, The Alliance has assessed the first 3 months of VBP Quality Rosters for the 2023 measurement year. Our application of the VBP Validation Model for FUH7 in June 2023 revealed a similar pattern to the 2022 measurement year where we found that performance scores for FUH were underrated[^7].

The results for FUH are under further investigation while we develop this model for AMM. The recurrence of this issue invites some concern. Learning from last year's results, we will carefully monitor FUH through the data, and track performance throughout the year, reporting our findings in real time. 

[^7]: Kivela, J.R. (June, 2023). Value Based Purchasing Report. *The Northern Arizona Regional Behavioral Health Alliance*.

## Hablemos de dato (Let's talk about the data)

This data model is simple in concept, but complex in design. The model compares Report Data from the HC VBP Quality Roster with Real Data from Health Choice adjudicated claims.

### What are the data sources?

 
|VBP Quality Roster | Adjudicated Claims |
|--|--|
| Health Choice receives this data from third party vendor, Cotivity.| A dataset of adjudicated claims queried directly from the HCA data warehouse.|            
| An excel workbook containing a roster of the members deemed eligible for VBP HEDIS NCQA measures including compliance status. | Records are gathered from the Claims and PBM databases, respectively. |
|  The Alliance receives individual rosters for each Alliance Provider.| Claims are extracted for all eligible service codes for the measurement year. |

### Is the data we use reliable?

| VBP Quality Roster | Adjudicated Claims |
|--|--|
| The outstanding question with the VBPQR is if the underlying data from the third party vendor, Cotivity, is accurate. | Claims data is of the highest quality as it is compiled and reviewed extensively by Health Choice for its own business purposes. |
| The report quality itself is very high as it is compiled by Health Choice Business Intelligence staff. | The quality of this data is also reviewed by state and federal regulation entities, like AZ Health Care Cost Containment System (AHCCCS). |
| The underlying data is what is under investigation. |

## The AMM Eligibility Model

We now embark upon building a dataset of eligible claims for the AMM measure. This process will be significantly more complicated than our previous model for FUH. The description of an eligible member event is very nuanced for AMM.

![ ](./images/AMMEligibilityModel_transparentbackground.png)

### An eligible member event is defined as follows:

-   The individual is an adult, aged 18 or older, **and**
-   A prescription was filled for an eligible antidepressant medication within the "Intake Period" ***(Index Prescription Start Date (IPSD))***, **and**
-   No other prescriptions were filled for eligible antidepressant medications within 105 days earlier than the IPSD ***(Test for Negative Medication History (NMH))***, **and**
-   The individual had a service for at least one of the eligible Major Depressive Disorders ***(Test for Major Depressive Disorder (MDDx))*** within 60 days (-60 days before, or +60 days after), **and**
-   The individual had continuous enrollment with Health Choice ***(Test for Continuous Enrollment (CE))*** for
    -   105 days before the IPSD, **and**
    -   231 days after the IPSD

Check out the visualization of the AMM Eligibility Model below. There are many components of this model with overlapping time frames. We will describe these steps in further detail in the next section.

![AMM Eligibility Model, NCQA HEDIS My2023](./data/DataModel/Graphic_AMMEligibilityModel2023.png){width=80%}


You may also think of the criteria as a tumbling bike lock. All four criteria must line up, *in the right order*, for the lock to open. Once unlocked, the resulting dataset es El Dato Verdad, the **Real Data**.

| **AMM Eligibility Model, Primary criteria:**<br>1.  Index Prescription Start Date (IPSD)<br>2.  Negative Medication History (NMH)<br>3.  Major Depressive Diagnosis (MDDx)<br>4.  Continuous Enrollment (CE) | ![](./images/bikelock.jpg){fig-alt="A tumbling bike lock representing the AMM Model" align="right" width="75%"} |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|

### The Index Prescription Start Date

::: panel-tabset

#### Description

The first criteria that must be met is the Index Prescription Start Date (IPSD). This is the date that an individual filled a prescription for one of the HEDIS My 2023 eligible antidepressant medications.

The IPSD is the anchor point for the rest of the eligibility measures. An individual must have an eligible IPSD in order to be assessed against the other tests for this measure.

The member must have an eligible prescription fill for an eligible antidepressant medication between the eligible dates indicated in the visualization above.

#### Procedure

In order to assess IPSD we did the following:

The 12 month Intake Period begins on May 1 of the *year prior* to the Measurement Year

-   The Measurement Year is: *`r InLineCode$MeasurementYear`*

-   The Intake Period is: *`r InLineCode$IntakePeriod`*

We extract claims from the *PBM database* at the Health Choice data warehouse for the given My2023 eligible medications dispensed within the Intake Period.

-   The NDC to GPI crosswalk must be used to determine eligible medications. The code to create the crosswalk is in the Appendix.

-   The minimum date of the Test for Negative Medication History must also be included in the PBM Claims query. The test for NMH is outlined below.

-   The Negative Medication History range is: *`r InLineCode$NegativeMedicationHistory`*

-   The prescription fill date in this range is called the ***Index Prescription Start Date (IPSD)***

#### Code

##### Extract claims data from HC data warehouse

```{sql}
#| label: PBMClaims
#| eval: false
#| include: true
#| echo: true
#| code-fold: show
#| warning: false
#| error: false

# Declare Date Range
Declare @start as date = '01-16-2022'
Declare @end as date = '04-30-2023'

SELECT
  pbm.clientID,
  id.PrimaryId,
  pbm.AsOfDate,
  pbm.dtefilled,
  pbm.GpiNumber,
  pbm.GPIClassification,
  pbm.prodname,
  pbm.genericnme,
  pbm.LabelName,
  pbm.GroupId,
  pbm.preslstnme,
  pbm.pbmrxclaimnbr,
  pbm.claimsts,
  pbm.AmtPaidFinal,
  pbm.productid,
  pbm.decimalqty,
  pbm.dayssupply,
  pbm.Gender,
  pbm.birthdte,
  pbm.mbrage
  
FROM
  PBM.dbo.HCICPharmacyClaimSummary pbm
  LEFT OUTER JOIN GlobalMembers.dbo.ClientIdPlus id ON pbm.clientID = id.AzAhcccsId
  
WHERE
  pbm.dtefilled BETWEEN @start AND @end
	AND
	pbm.mbrage >= 18
	AND
	pbm.GpiNumber IN (concatenated_values_GPI)

```

##### Conduct IPSD test

```{r}
#| label: IPSDTest
#| eval: false
#| include: true
#| echo: true 
#| code-fold: show
#| warning: false
#| error: false

# Import the raw claims
PBMClaims <- read.csv("./data/DataRaw_PBMClaims_2023-05-31.csv")

# Convert numeric variable to date
PBMClaims$dtefilled <- as.Date(as.character(PBMClaims$dtefilled), format = "%Y%m%d")

# Convert character variable to date
PBMClaims$AsOfDate <- as.Date(PBMClaims$AsOfDate, format = "%m/%d/%Y")

# write.csv(PBMClaims, "./data/output/PBMClaims.csv")

PBMClaims_NMHTest <- PBMClaims

# Convert the 'dtefilled' column to Date type if it's not already in the correct format
PBMClaims_NMHTest$dtefilled <- as.Date(PBMClaims_NMHTest$dtefilled)

# Define the start and end dates for IPSDTestResult
ip_start_date <- as.Date("2022-05-01")
ip_end_date <- as.Date("2023-04-30")

# Create the IPSDTestResult column based on the date conditions
# A result of "TRUE" indicates a positive IPSD
PBMClaims_NMHTest$IPSDTestResult <- ifelse(
  PBMClaims_NMHTest$dtefilled >= ip_start_date 
  & PBMClaims_NMHTest$dtefilled <= ip_end_date, "TRUE", "FALSE")

# write.csv(PBMClaims_NMHTest, "./data/output/PBMClaims_IPSDTest.csv")
```

#### In lay terms

Let me explain the provided R code in simple terms:

The code is performing some operations on a dataset called "PBMClaims" which contains information about claims. Here's what each part of the code does:

1. Importing the raw claims data: The code reads a CSV file ("DataRaw_PBMClaims_2023-05-31.csv") that contains the raw claims data.

2. Converting numeric variable to date: One column in the dataset, called "dtefilled," is in a numeric format representing dates. The code converts this column to a proper date format (year-month-day).

3. Converting character variable to date: Another column, called "AsOfDate," is in a character format representing dates. The code converts this column to a date format as well.

4. Creating a new dataset: A new dataset called "PBMClaims_NMHTest" is created, which is a copy of the original "PBMClaims" dataset.

5. Checking and converting the 'dtefilled' column to the correct date format: The code checks if the "dtefilled" column in the new dataset is already in the correct date format. If not, it converts it to the correct format (year-month-day).

6. Defining start and end dates for IPSDTestResult: Two dates, "ip_start_date" and "ip_end_date," are defined. These dates represent the start and end of a specific period called IPSD (Individual Prescription Supply Duration).

7. Creating the IPSDTestResult column: Based on the conditions of the "dtefilled" column falling within the IPSD period, a new column called "IPSDTestResult" is created in the new dataset. If a claim falls within the IPSD period, the corresponding value in the "IPSDTestResult" column is set to "TRUE"; otherwise, it is set to "FALSE".

In simple terms, the code is taking a dataset of claims, converting date columns to the proper format, and then identifying claims that fall within a specific period called IPSD. The result is a new column indicating whether each claim is within the IPSD period or not (ChatGPT, personal communication, July, 2023).

:::

### Test for Negative Medication History (NMH)

::: panel-tabset

#### Description

**The Test for Negative Medication History** determines if an individual is a new recipient of an antidepressant medication. An individual passes this test if they have **not** had a prescription for an antidepressant medication within 105 days **prior to** the IPSD.

**Be patient.** If you are replicating this project, this part of the program takes about 20 minutes to run. If you think about what it is doing, it is comparing every row of claims (\~147,000 rows) against the IPSD Intake Period range, and for all of the positive returns, comparing each one to each of the the original 147,000 rows for the NMH test.

Think about trying to guess the first 2 numbers on the bike lock, multiplied by 147,000. Anyway, it's a lot. Go grab some coffee.

#### Procedure

In order to conduct the test for Negative Medication History we took the following steps:

PBM Claims dataset is queried from the Health Choice data warehouse. This initial pull includes the entire potential range of PBM Claims for the Intake Period **and** the Negative Medication History period. Refer again to the visualization above for a visual aid.

Conduct the Negative Medication History Test

-   If the fill date is within the eligible Intake Period (IPSD), **and**
-   If there are no other fills within 105 days before the fill date, **then**
-   The test confirms the Negative Medication History (TRUE), **if not**
-   The test denies the Negative Medication History (FALSE)

Be patient. This part of the program takes about 20 minutes to run. If you think about what it is doing, it is comparing every row (147,000 rows) against the IPSD range, and for all the positive returns comparing each one to each of the the original 147,000 rows for the NMH test. Think about trying to guess the first 2 numbers on the bike lock. Anyway, it's a lot. Go grab some coffee.

#### Code

##### Negative Medication History Test

```{r}
#| label: NMHTest
#| eval: false
#| include: true
#| echo: true 
#| code-fold: show
#| warning: false
#| error: false

# Initialize NMHTestResult column with empty values
PBMClaims_NMHTest$NMHTestResult <- ""

# Loop through each row of the data frame
for (i in 1:nrow(PBMClaims_NMHTest)) {
  # Get the current ClientID, GpiNumber, and IPSDTest date
  current_client <- PBMClaims_NMHTest$clientID[i]
  current_gpi <- PBMClaims_NMHTest$GpiNumber[i]
  ipsd_test_date <- PBMClaims_NMHTest$dtefilled[i]
  
  # Find any matching observation with the same ClientID, GpiNumber, and a date less than 105 days before the IPSDTest date
  matching_observation <- PBMClaims_NMHTest$clientID[
    PBMClaims_NMHTest$clientID == current_client 
    & PBMClaims_NMHTest$GpiNumber == current_gpi 
    
## This is the problematic bit right here. Less than or greater than?
    & PBMClaims_NMHTest$dtefilled < ipsd_test_date - 105]
  
  # If there is no matching observation, update the NMHTestResult value for the current row
  if (length(matching_observation) != 0) {
    PBMClaims_NMHTest$NMHTestResult[i] <- "TRUE"
  } else {
    # If there is no matching observation, update the NMHTestResult value for the current row as "FALSE"
    PBMClaims_NMHTest$NMHTestResult[i] <- "FALSE"
  }
}


 write.csv(PBMClaims_NMHTest, "./data/output/PBMClaims_NMHTest_TestA.csv")

```

```{r}
#| label: NMHTest testing
#| eval: false
#| include: true
#| echo: true 
#| code-fold: show
#| warning: false
#| error: false

# Prompt:

# Load required packages
library(dplyr)
library(lubridate)

# Read the data from a file (assuming it's in a CSV format)
claims <- read.csv("./data/output/PBMClaims_IPSDTest.csv")

# Step 1: Select the desired columns
selected_columns <- claims %>%
  select(PrimaryId, GpiNumber, IPSDTestResult, dtefilled)

# Step 2: Create copies of the table
original <- selected_columns
test <- selected_columns

# Step 3: Join the two tables based on the given conditions
joined_table <- original %>%
  filter(IPSDTestResult) %>%
  inner_join(test, by = c("PrimaryId", "GpiNumber")) %>%
  select(-5)  # Drop the 5th column

# Step 4: Compare dtefilled.x with dtefilled.y and return the corresponding result
PBMClaims_NMHTest <- joined_table %>%
  rename(IPSDate = dtefilled.x, NMHTestDate = dtefilled.y)

PBMClaims_NMHTest <- PBMClaims_NMHTest %>%
  mutate(IPSDate = as.Date(IPSDate),
         NMHTestDate = as.Date(NMHTestDate),
         comparison_result = case_when(
           IPSDate < NMHTestDate ~ "IPSD is prior to test date",
           NMHTestDate < IPSDate ~ "IPSD is after test date",
           TRUE ~ NA_character_
         ))

# Step 5: Determine the number of days between IPSDate and NMHTestDate as an absolute value
PBMClaims_NMHTest <- PBMClaims_NMHTest %>% 
  mutate(days_between = abs(as.numeric(IPSDate - NMHTestDate)))

# Step 6: Determine "New Start" or "Positive Med History" based on days_between
PBMClaims_NMHTest <- PBMClaims_NMHTest %>%
  mutate(NMHTestResult = ifelse(days_between > 105, "New Start", "Positive Med History"))

# write to csv

 write.csv(PBMClaims_NMHTest, "./data/output/PBMClaims_NMHTest_TestB.csv")

```

```{r}
#| label: NMHTest testing 2
#| eval: false
#| include: true
#| echo: true 
#| code-fold: show
#| warning: false
#| error: false

# Prompt:

# Load required packages
library(dplyr)
library(lubridate)

# Read the data from a file (assuming it's in a CSV format)
claims <- read.csv("./data/output/PBMClaims_IPSDTest.csv")

# Step 1: Select the desired columns and add ProjectId column
selected_columns <- claims %>%
  select(clientID, PrimaryId, genericnme, GpiNumber, IPSDTestResult, dtefilled) %>%
  mutate(ProjectId = sample(1000000000:9999999999, nrow(.)))

# Step 2: Create copies of the table
original <- selected_columns
test <- selected_columns

# Step 3: Join the two tables based on the given conditions
joined_table <- original %>%
  filter(IPSDTestResult) %>%
  inner_join(test, by = c("clientID", "genericnme"))

joined_table <- joined_table |> 
  select("clientID", "PrimaryId.x", "ProjectId.x", "ProjectId.y", "genericnme", "dtefilled.x", "dtefilled")

# Step 4: Compare dtefilled.x with dtefilled.y and return the corresponding result
joined_table <- joined_table %>%
  rename(IPSDate = dtefilled.x, NMHTestDate = dtefilled.y)

# Step 4 Rogue: format as date
joined_table <- joined_table %>%
  mutate(IPSDate = as.Date(IPSDate),
         NMHTestDate = as.Date(NMHTestDate))

# Step 4 Rogue 1: Compare IPSD and NMH dates
         
joined_table <- joined_table %>%
  mutate(comparison_result = case_when(
           IPSDate < NMHTestDate ~ "IPSD is prior to NMHTest date",
           NMHTestDate < IPSDate ~ "IPSD is after test NMHTest date",
           TRUE ~ NA_character_
         ))

# Step 5: Determine the number of days between IPSDate and NMHTestDate as an absolute value
joined_table <- joined_table %>% 
  mutate(days_between = abs(as.numeric(IPSDate - NMHTestDate)))

# Step 6: Determine "New Start" or "Positive Med History" based on days_between
joined_table <- joined_table %>%
  mutate(NMHTestResult = ifelse(days_between > 105, "New Start", "Positive Med History"))

 write.csv(joined_table, "./data/output/PBMClaims_NMHTest_TestB.csv")

PBMClaims_NMHTest <- joined_table

# Step 7: Filter out cases with "Positive Med History"
# filtered_table <- joined_table %>%
#   filter(NMHTestResult != "Positive Med History")

```
#### In lay terms

The code is performing an analysis on a dataset called "PBMClaims_NMHTest" that contains information about claims. Here's what each part of the code does:

1. Initializing the NMHTestResult column: The code adds a new column called "NMHTestResult" to the dataset and initializes it with empty values.

2. Looping through each row of the dataset: The code goes through each row of the dataset one by one.

3. Extracting relevant information: For each row, the code extracts the ClientID, GpiNumber, and IPSDTest date.

4. Searching for matching observations: The code looks for any other rows in the dataset that have the same ClientID and GpiNumber but a date that is less than 105 days before the IPSDTest date of the current row.

5. Checking for a matching observation: If a matching observation is found, it means that there is a claim for the same ClientID and GpiNumber within the specified time frame. In this case, the "NMHTestResult" value for the current row is set to "TRUE".

6. Handling no matching observation: If no matching observation is found, it means that there is no claim for the same ClientID and GpiNumber within the specified time frame. In this case, the "NMHTestResult" value for the current row is set to "FALSE".

In simple terms, the code is going through each claim in the dataset and checking if there was a previous claim for the same client and medication within a specific time frame (105 days before the current claim). If such a claim exists, the "NMHTestResult" column is marked as "TRUE" for that claim. Otherwise, it is marked as "FALSE".

*Generated by ChatGPT
:::

::: callout-tip

## Fantastico!

Wow! I can't believe that we actually pulled that off!

Now we have our confirmation of IPSD and NMH.

***Two down, two to go. Awesome!***

:::

### Test for Major Depressive Disorder

::: panel-tabset

#### Description

The next stop on the road is to confirm that the individual had a service for at least one of the eligible Major Depressive Disorders (MDDx) within 60 days (-60 days before, or +60 days after) of the IPSD.

A diagnosis of Major Depressive Disorder is identified by cross referencing members with an eligible IPSD and NMH (using PBM records), against behavioral health claims records for MDD. Individual identifying data is used to join PBM records with claims records, and make the comparison.

#### Procedure

Behavioral health claims are extracted from the HCA data warehouse for all cases where there was an eligible Major Depressive Disorder Diagnosis.

Conduct the MDDx Test by comparing each of the cases identified by PBMClaims with any of the MDDx claims.

The result is a big data solution consisting of over 1.7 million possible combinations. We now have grown our dataset to include an assessment for eligible behavioral health services.

#### Code

##### Import the HEDIS My2023 value set for eligible Major Depressive Diagnoses

```{r}
#| label: MajDepressiveDxTest
#| eval: false
#| include: true
#| echo: true 
#| warning: false
#| error: false

# Import the Value Set List
ValueSetListMy2023 <- read.csv("./data/ValueSetListMy2023.csv")

# Create a list of the diagnosis codes
MDDxList <- ValueSetListMy2023$Code

# Concatenate for adding to sql
concatenated_values_MDDx <- paste0("(",    ValueSetListMy2023$Code, ")", collapse = ", ")

```

##### Query the Health Choice data warehouse for eligible MDDx claims

```{sql}
#| label: MajDepressiveDxTestQuery
#| eval: false
#| include: true
#| echo: true 
#| warning: false
#| error: false
-- Declare start and end variables
DECLARE @start DATE = '2022-03-22';
DECLARE @end DATE = '2023-10-30';

-- Check if the temporary table exists and drop it if it does
IF OBJECT_ID('tempdb..#ValueSetListMy2023') IS NOT NULL
    DROP TABLE #ValueSetListMy2023;

-- Create a temporary table
CREATE TABLE #ValueSetListMy2023 (Code VARCHAR(100) COLLATE SQL_Latin1_General_CP1_CI_AS);

-- Insert values into the temporary table
INSERT INTO #ValueSetListMy2023 (Code)
VALUES ('101'), ('100'), ('207'), ('116'), ('126'), ('136'), ('146'), ('156'), ('110'), ('120'), 
       ('130'), ('140'), ('150'), ('160'), ('170'), ('190'), ('200'), ('210'), ('1000'), 
       ('213'), ('214'), ('206'), ('202'), ('111'), ('121'), ('131'), ('141'), ('151'), 
       ('211'), ('171'), ('172'), ('173'), ('174'), ('122'), ('132'), ('142'), ('152'), 
       ('112'), ('117'), ('127'), ('137'), ('147'), ('157'), ('119'), ('129'), ('139'), 
       ('149'), ('159'), ('169'), ('219'), ('209'), ('179'), ('199'), ('113'), ('123'), 
       ('133'), ('143'), ('153'), ('203'), ('114'), ('124'), ('134'), ('144'), ('154'), 
       ('204'), ('212'), ('118'), ('128'), ('138'), ('148'), ('158'), ('1002'), ('1001'), 
       ('167'), ('164'), ('191'), ('192'), ('193'), ('194'), ('201'), ('208'), ('F32.0'), 
       ('F32.1'), ('F32.2'), ('F32.3'), ('F32.4'), ('F32.9'), ('F33.0'), ('F33.1'), ('F33.2'), 
       ('F33.3'), ('F33.41'), ('F33.9'), ('14183003'), ('2618002'), ('726772006'), 
       ('320751009'), ('36923009'), ('370143000'), ('1.08111E+16'), ('1.08112E+16'), 
       ('42925002'), ('69392006'), ('63778009'), ('25922000'), ('87512008'), ('79298009'), 
       ('1.6266E+16'), ('40379007'), ('720455008'), ('720454007'), ('720451004'), ('832007'), 
       ('15639000'), ('1.62668E+16'), ('18818009'), ('719592004'), ('720453001'), 
       ('720452006'), ('66344007'), ('38694004'), ('39809009'), ('319768000'), ('71336009'), 
       ('268621008'), ('191610000'), ('191611001'), ('191613003'), ('1.62646E+16'), 
       ('1.62649E+16'), ('1.62648E+16'), ('450714000'), ('73867007'), ('33736005'), 
       ('60099002'), ('75084000'), ('2.51E+11'), ('430852001'), ('77911002'), ('20250007'), 
       ('76441001'), ('1.6267E+16'), ('2.81E+11'), ('28475009'), ('33078009'), ('15193003'), 
       ('36474008'), ('191604000');

-- Query to retrieve data from the claims.dbo.shcavos table
SELECT DISTINCT
	shcavos.primaryID, 
	id.BCBSMedicaidId AS MemberID,
    shcavos.begDate,
    shcavos.PrimaryDiagnosis,
    shcavos.Dx1,
    shcavos.Dx2,
    shcavos.Dx3,
    shcavos.Dx4,
    shcavos.Dx5,
    shcavos.Dx6,
    shcavos.Dx7,
    shcavos.Dx8,
    shcavos.Dx9,
    shcavos.Dx10,
    shcavos.Dx11,
    shcavos.Dx12,
    CASE WHEN v.Code IS NOT NULL THEN 'True' ELSE 'False' END AS MatchFound
FROM claims.dbo.shcavos AS shcavos
LEFT JOIN GlobalMembers.dbo.ClientIdPlus id ON shcavos.primaryID = id.primaryID
LEFT JOIN #ValueSetListMy2023 AS v ON shcavos.PrimaryDiagnosis COLLATE SQL_Latin1_General_CP1_CI_AS = v.Code
                                    OR shcavos.Dx1 COLLATE SQL_Latin1_General_CP1_CI_AS = v.Code
                                    OR shcavos.Dx2 COLLATE SQL_Latin1_General_CP1_CI_AS = v.Code
                                    OR shcavos.Dx3 COLLATE SQL_Latin1_General_CP1_CI_AS = v.Code
                                    OR shcavos.Dx4 COLLATE SQL_Latin1_General_CP1_CI_AS = v.Code
                                    OR shcavos.Dx5 COLLATE SQL_Latin1_General_CP1_CI_AS = v.Code
                                    OR shcavos.Dx6 COLLATE SQL_Latin1_General_CP1_CI_AS = v.Code
                                    OR shcavos.Dx7 COLLATE SQL_Latin1_General_CP1_CI_AS = v.Code
                                    OR shcavos.Dx8 COLLATE SQL_Latin1_General_CP1_CI_AS = v.Code
                                    OR shcavos.Dx9 COLLATE SQL_Latin1_General_CP1_CI_AS = v.Code
                                    OR shcavos.Dx10 COLLATE SQL_Latin1_General_CP1_CI_AS = v.Code
                                    OR shcavos.Dx11 COLLATE SQL_Latin1_General_CP1_CI_AS = v.Code
                                    OR shcavos.Dx12 COLLATE SQL_Latin1_General_CP1_CI_AS = v.Code
WHERE shcavos.begDate BETWEEN @start AND @end
AND CASE WHEN v.Code IS NOT NULL THEN 'True' ELSE 'False' END = 'True';

# Save the result as MDDxClaims_dateslug.csv

```

##### Load data from the claims query results into the data model

```{r}
#| label: MDDxClaimsImport
#| eval: false
#| include: true
#| echo: true 
#| warning: false
#| error: false

# Import the MDDxClaims from excel
MDDxClaims <- read_csv("./data/MDDxClaims_20230602.csv")

# write.csv(MDDxClaims, "./data/output/MDDxClaims.csv")
```

##### Conduct the MDDx Test

```{r}
#| label: MDDxClaims1
#| eval: false
#| include: true
#| echo: true 
#| warning: false
#| error: false

# Step 1: Load the MDDxClaims data
MDDxClaimsTest <- MDDxClaims %>%
  rename(PrimaryId = primaryID)


# Step 2: Add a new column "ProjectId_MDD" with random 10-digit numbers
MDDxClaimsTest <- MDDxClaimsTest %>%
  mutate(ProjectId_MDD = sample(1000000000:9999999999, nrow(.))) |>
  filter(!is.na(begDate)) |> 
  rename("MDDxDate" = begDate)

# Step 2a: Shed a little extra weight
MDDxClaimsTest <- MDDxClaimsTest %>%
  select(PrimaryId, MemberID, ProjectId_MDD, MDDxDate, PrimaryDiagnosis)


# Step 3: Filter out cases with "Positive Med History"
filtered_table <- PBMClaims_NMHTest %>%
  filter(NMHTestResult == "New Start")

filtered_table <- filtered_table |> 
  rename(PrimaryId = PrimaryId.x)

# Step 4: Merge PBMClaims_NMHTest and MDDxClaims on PrimaryId, keeping all rows and columns
PBMClaims_MDDxMerge <- merge(x = filtered_table, y = MDDxClaimsTest, by = "PrimaryId", all.x = TRUE)

# Diverge to counting methodology

# Step 5: Determine the number of days between IPSDate and NMHTestDate as an absolute value
PBMClaims_MDDxTest <- PBMClaims_MDDxMerge %>% 
  mutate(days_between_MDDx = abs(as.numeric(IPSDate - MDDxDate)))

# Step 6: Determine "New Start" or "Positive Med History" based on days_between
PBMClaims_MDDxTest <- PBMClaims_MDDxTest %>%
  mutate(MDDxTestResult = ifelse(days_between_MDDx < 60, "Positive MDDx", "No MDDx")) |>
  filter(!is.na(MDDxDate))




# write.csv(PBMClaims_MDDxTest, "./data/output/PBMClaims_MDDxTest.csv")

```

#### In lay terms

1. The code is loading the data from the "MDDxClaims" dataset and assigning it to a new variable called "MDDxClaimsTest". It also renames the column "primaryID" to "PrimaryId" in the new dataset.

2. The code merges two datasets, "PBMClaims_NMHTest" and "MDDxClaimsTest", based on the common column "PrimaryId". The result is stored in a new dataset called "PBMClaims_MDDxMerge". It includes all rows and columns from both datasets.

3. The code creates a copy of the merged dataset and assigns it to a new variable called "PBMClaims_MDDxTest".

4. A new column called "MDDxTestResult" is added to the "PBMClaims_MDDxTest" dataset. The column is populated based on a test condition. If the absolute difference between the "begDate" and "dtefilled" columns is less than or equal to 60 (in terms of days), the value in "MDDxTestResult" is set to TRUE; otherwise, it is set to FALSE.

5. The code selects specific columns of interest from the "PBMClaims_MDDxTest" dataset and creates a new dataset called "PBMClaims_MDDxTest" with the selected columns. The column names are appended with "_MDDXTest".

In summary, this code loads and manipulates two datasets, "PBMClaims_NMHTest" and "MDDxClaims", and combines them based on a common column. It then performs a test using date values and creates a new dataset with selected columns for further analysis or reporting.
:::

::: callout-tip

## Woo hoo! It worked!

We have now determined that there is:

1.  A valid Index Prescription Start Date (IPSD)
2.  A Negative Medication History (NMH)
3.  A valid service for Major Depressive Disorder (MDDx)
:::

### Test for Continuous Enrollment

::: panel-tabset

#### Description

Ok, three down, one to go. Awesome!

Now that we have confirmed IPSD, NMH, and MDDx, we will search for the member information in our enrollment records. We then determine if there was an eligible enrollment period associated with the prescription fill.

An prescription fill is eligible if the individual had continuous enrollment for -105 days before the IPSD, **AND** +231 days after the IPSD.

#### Procedure

Compare the Index Prescription Start Date against the member enrollment roster developed in the Alliance Progress Report [^8]. We use the full global members roster in this case because we are not filtering for active membership only. Individuals who are currently disenrolled are included in the dataset as well.

[^8]: Kivela, J.R., McMillian, J., Tewa, V. (June 2023). The Alliance Progress Report. *The Northern Arizona Regional Behavioral Health Alliance*.

#### Code

##### Test for Continuous Enrollment

```{r}
#| label: Continuous Enrollment
#| eval: false
#| include: true
#| echo: true 
#| warning: false
#| error: false


# Import the original global members roster
GlobalMembers_orig <- read_xlsx("./data/data_original_glblmbrs_2023-05-01_globalMembersRoster.xlsx", sheet = "Sheet1")

# Create a working copy and rename "x" to PrimaryId
GlobalMembers <- GlobalMembers_orig |> 
  rename("PrimaryId" = x)

# Using the PBMClaims_MDDxTest table as a base, attach Global Members data by PrimaryId
PBMClaims_CETest1 <- merge(x = PBMClaims_MDDxTest,
              y = GlobalMembers,
              by = "PrimaryId",
              all = TRUE) 

PBMClaims_CETest1 <- PBMClaims_CETest1 |> 
  filter(MDDxTestResult == "Positive MDDx")



# Continuous Enrollment Test (Pre)


# write.csv(PBMClaims_CE_Test, "./data/output/PBMClaims_CE_Test.csv")

```

#### In lay terms

Here's an explanation of the provided code:

1. Import the original global members roster from an Excel file and store it in the variable `GlobalMembers_orig`.
2. Create a working copy of the global members roster and rename the column named "x" to "PrimaryId". Store the modified copy in the variable `GlobalMembers`.
3. Merge the `PBMClaims_MDDxTest` table with the `GlobalMembers` data based on the "PrimaryId" column, keeping all rows from both tables. The merged result is stored in the variable `PBMClaims_CETest1`.
4. Perform a continuous enrollment test for the "Pre" period:
   - Calculate the difference in days between the "bhhEffectiveDate" and the "dtefilled" columns, but only if the "bhhEffectiveDate" is earlier than the "dtefilled" date. Store the result in the column named "CEDaysDiff_Pre".
   - Create a new column named "CEDaysDiff_PreTest" that indicates whether the "CEDaysDiff_Pre" is greater than 105.
5. Perform a continuous enrollment test for the "Post" period:
   - Calculate the difference in days between the "bhhEffectiveDate" and the "dtefilled" columns, but only if the "bhhEffectiveDate" is earlier than the "dtefilled" date. Store the result in the column named "CEDaysDiff_Post".
   - If the "disenrollmentDate" is null (meaning the person is still enrolled), calculate the difference in days between the "dtefilled" date and today's date. If the "disenrollmentDate" is not null (meaning the person is disenrolled), calculate the difference in days between the "dtefilled" date and the "disenrollmentDate". Store the result in the column named "CEDaysDiff_Post".
   - Create a new column named "CEDaysDiff_PostTest" that indicates whether the "CEDaysDiff_Post" is greater than 231.
6. Conduct a test to check if both the pre and post conditions are met. Create a new column named "CETestResult" that is set to TRUE if both "CEDaysDiff_PostTest" and "CEDaysDiff_PreTest" are TRUE, and FALSE otherwise.
7. Select specific choice variables from the `PBMClaims_CETest1` table and store the result in the variable `PBMClaims_CE_Test`.
8. Sort the data in `PBMClaims_CE_Test` in descending order based on the "PrimaryId", "dtefilled", and "IPSDTestResult" columns.

In summary, the code performs tests related to continuous enrollment for pre and post periods based on various date columns. It creates new columns to store the results of these tests and selects specific variables of interest for further analysis. The data is then sorted based on certain columns.

:::


### AMM Eligibility Model, Check!

We have now determined that there is:

1.  A valid Index Prescription Start Date,
2.  A Negative Medication History,
3.  A valid service for Major Depressive Disorder, and
4.  A valid period of enrollment.

The resulting dataset presents a list of members with indicators for each of the respective tests. A person with a prescription fill that met each of the conditions is included, and will be compared with the VBP Quality Roster Data.

::: panel-tabset
```{r}
#| label: AMM Eligibility Test
#| eval: false
#| include: true
#| echo: true 
#| warning: false
#| error: false
# Conduct the AMM Eligibility Test

# Convert the variables to logical if needed
PBMClaims_CE_Test$IPSDTestResult <- as.logical(PBMClaims_CE_Test$IPSDTestResult)
PBMClaims_CE_Test$NMHTestResult <- as.logical(PBMClaims_CE_Test$NMHTestResult)
PBMClaims_CE_Test$MDDxTestResult <- as.logical(PBMClaims_CE_Test$MDDxTestResult)
PBMClaims_CE_Test$CETestResult <- as.logical(PBMClaims_CE_Test$CETestResult)

# Create the new column "AMMEligibilityTest"
PBMClaims_CE_Test$AMMEligibilityTestResult <- with(PBMClaims_CE_Test, IPSDTestResult & NMHTestResult & MDDxTestResult & CETestResult)

# Select Choice variables:
AMMEligibilityTest <- PBMClaims_CE_Test[, c("PrimaryId", "clientID", "MemberID", "dtefilled", "AMMEligibilityTestResult", "IPSDTestResult", "NMHTestResult", "MDDxTestResult", "CETestResult", "begDate", "PrimaryDiagnosis", "GpiNumber", "LabelName", "preslstnme", "decimalqty", "dayssupply", "CEDaysDiff_Pre", "CEDaysDiff_Post", "AmtPaidFinal")]

# Sort the data
AMMEligibilityTest <- AMMEligibilityTest %>%
  arrange(desc(PrimaryId), desc(dtefilled), AMMEligibilityTestResult)

# write.csv(AMMEligibilityTest, "./data/output/AMMEligibilityTest.csv")

# write.csv(AMMEligibilityTest, "./data/output/AMMEligibilityTest_Copy.csv")
 
# Filter AMMEligibilityTest for only those cases that are eligible
AMMEligibilityTest_AllEligible <- AMMEligibilityTest |> filter(AMMEligibilityTestResult == TRUE)

# write.csv(AMMEligibilityTest_AllEligible, "./data/output/AMMEligibilityTest_AllEligible.csv")

```
:::

## Value-based Purchasing Quality Reports (VBP QR)

Now that we have a reliable dataset for the Real Data (adjudicated claims), we will introduce the Report Data (VBP Quality Roster). We will import and aggregate the Report Data, clean it up, and then compare it with Real Data.

::: panel-tabset

#### Description

The VBP Roster comes to us from Health Choice, but the underlying data is analyzed and produced by a third party vendor, Cotivity.

The Alliance receives a separate report for each of the Alliance Providers. This procedure outlines how the VBP QR is input from the original report and transformed to actionable data.

The resulting table is a complete, cleaned roster of all of the individual member events, for all of the VBP Measures, for all of the Alliance Providers. It is then filtered to assess each measure individually.

***This table is used to construct the VBP Quality Report Dashboard as well as the Alliance Progress Report***

#### Procedure

The most recent Value-based Purchasing (VBP) Quality Roster (`r InLineCode$ReportDate`) for each Alliance Provider (AP) was aggregated. The Roster page from the excel data model was extracted from each of the individual reports and compiled into one aggregate data frame that contains the results of all Alliance Providers.

The VBP Report data is cumulative over the VBP measurement year `r InLineCode$MeasurementYear` and has a 60 day claims lag, such that the `r InLineCode$ReportDate` VBP QR contains claims adjudicated through `r InLineCode$ClaimsAdjudicatedThrough`.

The *SubMeasureID* variable was filtered for this study to only include the *AMM2* measure, and a list of the unduplicated Member IDs.

#### Code

##### Import VBPQR Data

```{r}
#| label: Import VBPQR data
#| eval: false
#| include: true
#| echo: true 
#| warning: false
#| error: false

# Import the unaltered VBP report, "Detail" sheet, as received from HCA
# 5/1/23 sheet = "Detail" was change by HCA to sheet = "Roster"
vbp_cbi   <-  read_xlsx("./data/VBPReports/Quality/vbpbhh_report_2023-05-31_94-2880847_Community_Bridges_HCA_BHH_VBP_Quality_Roster.xlsx", sheet = "Roster")
##  vbp_cbi <- vbp_cbi [,-1] 
##  colnames(vbp_cbi) <- c("BCBSAZ Health Choice" ,"...2", "...3", "...4", "...5", "...6", "...7", "...8", "...9")
vbp_cpih  <-  read_xlsx("./data/VBPReports/Quality/vbpbhh_report_2023-05-31_86-0215065_Change_Point_Integrated_Health_HCA_BHH_VBP_Quality_Roster.xlsx", sheet = "Roster")
vbp_lcbhc <-  read_xlsx("./data/VBPReports/Quality/vbpbhh_report_2023-05-31_86-0250938_Little_Colorado_Behavioral_Health_HCA_BHH_VBP_Quality_Roster.xlsx", sheet = "Roster")
vbp_mmhc  <-  read_xlsx("./data/VBPReports/Quality/vbpbhh_report_2023-05-31_86-0214457_Mohave_Mental_Health_HCA_BHH_VBP_Quality_Roster.xlsx", sheet = "Roster")
vbp_ph    <-  read_xlsx("./data/VBPReports/Quality/vbpbhh_report_2023-05-31_86-0206928_Polara_Health_HCA_BHH_VBP_Quality_Roster.xlsx", sheet = "Roster")
vbp_sbhs  <-  read_xlsx("./data/VBPReports/Quality/vbpbhh_report_2023-05-31_86-0290033_Southwest_Behavioral_Health_HCA_BHH_VBP_Quality_Roster.xlsx", sheet = "Roster")
vbp_shg   <-  read_xlsx("./data/VBPReports/Quality/vbpbhh_report_2023-05-31_86-0207499_Spectrum_Health_Group_HCA_BHH_VBP_Quality_Roster.xlsx", sheet = "Roster")
vbp_tgc   <-  read_xlsx("./data/VBPReports/Quality/vbpbhh_report_2023-05-31_86-0223720_The_Guidance_Center_HCA_BHH_VBP_Quality_Roster.xlsx", sheet = "Roster")

# Pro Tip
# if any of the tables pick up rogue columns...
# vbp_cbi <- vbp_cbi [,-1] 
# colnames(vbp_cbi) <- c("BCBSAZ Health Choice" ,"...2", "...3", "...4", "...5", "...6", "...7", "...8", "...9")

# Bind the Details sheet from all providers into one table
DataRaw_VBPQR_AllAPsCombined <- rbind(
  vbp_cbi,
  vbp_cpih,
  vbp_lcbhc,
  vbp_mmhc,
  vbp_ph,
  vbp_sbhs,
  vbp_shg,
  vbp_tgc
)

# # write to csv
# date of file = date of VBP QR report
# write.csv(DataRaw_VBPQR_AllAPsCombined, "./data/output/2023-05-31_DataRaw_VBPQR_AllAPsCombined.csv")
```

##### Wrangle Said Data

```{r}
#| label: Wrangle VBPQR data
#| eval: false
#| include: true
#| echo: true 
#| warning: false
#| error: false

# create a safe copy of the original data
VBPQR_AllAPsCombined_Cleaned <- DataRaw_VBPQR_AllAPsCombined

# Filter out superfluous rows of nonsense data
VBPQR_AllAPsCombined_Cleaned <- VBPQR_AllAPsCombined_Cleaned |>  
  filter(`...2` != "NA")

# Set column names to headers, which get imported on row 1 #5/2/23 - updated from "6" 
colnames(VBPQR_AllAPsCombined_Cleaned) <- VBPQR_AllAPsCombined_Cleaned [1,] 

# Remove the first row of data that headers
VBPQR_AllAPsCombined_Cleaned <- VBPQR_AllAPsCombined_Cleaned[-1,]

# 5/1/23 - Create SubMeasureID
VBPQR_AllAPsCombined_Cleaned$`SubMeasure ID` <- substr(VBPQR_AllAPsCombined_Cleaned$Measure, 1, 3)

# write.csv(VBPQR_AllAPsCombined_Cleaned, "./data/output/VBPQualityRoster.csv")
# write.csv(VBPQR_AllAPsCombined_Cleaned, "C:/Users/KGLtd/OneDrive - The NARBHA Institute/Documents - Data Force/Projects/AllianceIntranetSupport/data/output/VBPQualityRoster.csv")

```

```{r}
#| label: Wrangle VBPQR data 2
#| eval: false
#| include: true
#| echo: true 
#| warning: false
#| error: false

# create a duplicate at this phase to be used in later evaluation
VBPQR_AllAPsCombined_Cleaned2 <- VBPQR_AllAPsCombined_Cleaned |> 
  filter(`SubMeasure ID` == "AMM")

# write.csv(VBPQR_AllAPsCombined_Cleaned2, "./data/output/VBPQualityRoster_AMM.csv")

# Isolate member ID for the validation
VBPQR_AllAPsCombined_Cleaned <- VBPQR_AllAPsCombined_Cleaned |> 
  filter(`SubMeasure ID` == "AMM") |> 
  select(`Member ID`)

# write.csv(VBPQR_AllAPsCombined_Cleaned, "./data/output/VBP_Validation.csv")
```

#### In lay terms

##### Import VBPQR data:
1. The code imports several VBP (Value-Based Purchasing) reports from different providers. These reports contain information about the quality of services related to behavioral health.
2. Each provider's report is stored in a separate variable, such as vbp_cbi, vbp_cpih, vbp_lcbhc, etc. These variables represent the data tables within the reports.
3. The code reads the "Roster" sheet from each provider's report file and stores the data in the corresponding variable.
4. All the provider data tables are then combined into one table called DataRaw_VBPQR_AllAPsCombined. This table contains the combined data from all providers.

##### Wrangle Said Data:
1. A copy of the combined data table, DataRaw_VBPQR_AllAPsCombined, is created for cleaning and manipulation.
2. Superfluous rows of irrelevant data are filtered out from the table.
3. The first row of the table, which contains headers, is used to set the column names for the table.
4. The first row of data (headers) is removed from the table.
5. A new column called "SubMeasure ID" is created by extracting the first three characters from the "Measure" column.
6. The cleaned and manipulated data table is saved as VBPQR_AllAPsCombined_Cleaned.

##### Wrangle VBPQR data 2:
1. Another copy of the cleaned data table, VBPQR_AllAPsCombined_Cleaned, is created for further evaluation.
2. This copy is filtered to include only rows where the "SubMeasure ID" is "AMM".
3. The filtered data table is saved as VBPQR_AllAPsCombined_Cleaned2.
4. The original cleaned data table is further filtered to isolate the "Member ID" column for validation.
5. The filtered data table is saved as VBPQR_AllAPsCombined_Cleaned for validation purposes.

In summary, the code imports VBPQR reports from different providers, combines them into a single table, and performs data cleaning and manipulation to prepare the data for analysis and validation. The resulting data tables are saved for further use (ChatGPT, personal communication, July, 2023).

:::

# AMM Validation Data Modeling

The Validation Model for AMM is *Conceptual* and *Logical*. This model will help to define the systems surrounding measurement of AMM. This model also outlines a set of logical rules and structures of data to be used across agencies and measures.

![NCQA HEDIS My2023 Validation Model for AMM](./images/FullAMMValidationModel.png)

::: panel-tabset

## Description of Test Design

This model extracts data from multiple VBP Quality Reports and aggregates it them into a single dataset. It also collects adjudicated claims data from the Health Choice data warehouse.

The 2 datasets are then joined to create the Validation Matrix, comparing VBP cases against adjudicated claims.

***The Validation Matrix is the table that will be used for evaluation! Phew! :)***

## Procedure

### Report Data: VBP Quality Roster Reports

**The VBPQR_AllAPsCombined_Cleaned** data frame was loaded to the test model. The data was summarized, per member, by counting the instances of eligibility per member. This ultimately creates a vector of unduplicated Member IDs called ***VBP_Unduplicated***.

### Real Data: Adjudicated Claims

The ***AMMEligibilityTest*** claims data frame was loaded to the test model. It was then summarized by counting the instances of HEDIS My2022 eligible claims per member. This creates a vector of unduplicated Member IDs called ***AMMClaims_Unduplicated***.

### The Validation Matrix

The ***VBP_Unduplicated*** and the ***AMMClaims_Unduplicated*** are joined, and all rows of data from both variables are included, regardless of match. The resulting data frame is called ***Validation_Matrix***.

-   This table contains all of the unduplicated VBPQR MemberIDs, *and* all of the unduplicated Claims MemberIDs.

The data is assessed for cases where a VBP MemberID is validated against a Claims MemberID. 
-   A positive result is called "Match", and a negative result is called "NoMatch"

<span style="color:purple;">The Validation_Matrix is the table that will be used for evaluation! Phew! :)</span>


## Code

```{r}
#| label: Modeling VBP Unduplicated
#| eval: false
#| include: true
#| echo: true 
#| warning: false
#| error: false

VBP_Unduplicated <- VBPQR_AllAPsCombined_Cleaned |> 
  group_by(`Member ID`) |> 
  rename("MemberID" = `Member ID`) |> 
  count()

# write.csv(VBP_Unduplicated, "./data/output/VBP_Unduplicated.csv")
```

```{r}
#| label: Modeling Claims Unduplicated
#| eval: false
#| include: true
#| echo: true 
#| warning: false
#| error: false

AMMClaims_Unduplicated <- AMMEligibilityTest_AllEligible |> 
  filter(MemberID != "NULL") |> 
  group_by(MemberID) |> 
  count()

# write.csv(AMMClaims_Unduplicated, "./data/output/AMMClaims_Unduplicated.csv")


```

```{r}
#| label: Modeling Validation Matrix
#| eval: false
#| include: true
#| echo: true 
#| warning: false
#| error: false

Validation_Matrix <- 
  merge(x = VBP_Unduplicated,
        y = AMMClaims_Unduplicated,
        by = "MemberID",
        all = TRUE) |> 
  rename("VBP" = n.x,
         "claims" = n.y) |> 
  mutate(Match = if_else((is.na(VBP) | is.na(claims)), "NoMatch", "Match"))

# write.csv(Validation_Matrix, "./data/output/ValidationMatrix.csv")
```

## Summary

### VBP Reports

-   **8** VBP report details sheets were imported, merged and cleaned, creating ***DataRaw_VBPQR_AllAPsCombined*** with `r nrow(DataRaw_VBPQR_AllAPsCombined)` observations of `r ncol(DataRaw_VBPQR_AllAPsCombined)` variables.

-   ***VBPQR_AllAPsCombined_Cleaned*** was created from the master to isolate instances of FUH7, and then select for *Member ID*, ultimately yielding `r nrow(VBPQR_AllAPsCombined_Cleaned)` observations of MemberIDs.

### Claims

-   Pharmacy Records from HCA Pharmacy Benefit Manager were queried, including all claims for eligible AMM medications, for the date range including the IPSD and the NMH Tests. There were `r PBMClaims |> nrow()` records.
-   Behavioral Health records were querried from the HCA behavioral health claims system for the date range including all eligible dates for a Major Depressive Diagnosis. There were `r MDDxClaims |> nrow()` records.

### Enrollment
-   Enrollment data was queried from the HCA Global Members database for the entire range of possible enrollment and disenrollment dates. There were `r GlobalMembers_orig |> nrow()` members identified.

## In lay terms

This R code is performing some data manipulation and analysis tasks. Let me break it down for you:

The first block of code, labeled "Modeling VBP Unduplicated," is doing the following:

1. The dataset *VBPQR_AllAPsCombined_Cleaned* is being grouped by the column "Member ID."
2. The column "Member ID" is then renamed to "MemberID."
3. The number of occurrences of each unique "MemberID" is counted and stored in a new dataset called *VBP_Unduplicated*.
4. There is a commented out line (*# write.csv(...)*) that suggests the intention to # write the *VBP_Unduplicated* dataset to a CSV file.

The second block of code, labeled "Modeling Claims Unduplicated," performs the following actions:

1. The dataset *AMMEligibilityTest_AllEligible* is filtered to exclude any rows where the "MemberID" is equal to "NULL."
2. The dataset is then grouped by the "MemberID" column.
3. The number of occurrences of each unique "MemberID" is counted and stored in a new dataset called *AMMClaims_Unduplicated*.
4. There is a commented out line (*# write.csv(...)*) that suggests the intention to # write the *AMMClaims_Unduplicated* dataset to a CSV file.

The third block of code, labeled "Modeling Validation Matrix," combines the previous two datasets (*VBP_Unduplicated* and *AMMClaims_Unduplicated*) to create a validation matrix:

1. The datasets *VBP_Unduplicated* and *AMMClaims_Unduplicated* are merged based on the common column "MemberID." The resulting merged dataset is stored in *Validation_Matrix*.
2. The columns *n.x* and *n.y* (which represent the counts from the previous datasets) are renamed to "VBP" and "claims," respectively.
3. A new column called "Match" is added to the *Validation_Matrix* dataset. The "Match" column is populated with the value "NoMatch" if either the "VBP" or "claims" values are missing (*NA*), and "Match" otherwise.
4. There is a commented out line (*# write.csv(...)*) that suggests the intention to # write the *Validation_Matrix* dataset to a CSV file.

In summary, this code takes two datasets (*VBPQR_AllAPsCombined_Cleaned* and *AMMEligibilityTest_AllEligible*), performs data manipulation and aggregation operations on them, and creates a validation matrix (*Validation_Matrix*) to determine matches and mismatches based on the "MemberID" column. The intention is to potentially # write the resulting datasets to CSV files for further analysis or storage.
:::

# Results

## Validating Report Data using Real Data

In order to conduct the validation test, all records from VBP_Unduplicated are included, while Claims_Unduplicated records are only included if they have a positive match with VBP_Unduplicated on the variable *Member ID*.

```{r}
#| label: Compliance Table
#| eval: false
#| include: false
#| echo: true 
#| warning: false
#| error: false

# Rename the member Id column
VBPQR_AllAPsCombined_Cleaned2 <- VBPQR_AllAPsCombined_Cleaned2 %>%
  rename(MemberID = `Member ID`)

# transform VBP_Rep_Comp for compliance
Compliance <-
  merge(x = Validation_Matrix,
        y = VBPQR_AllAPsCombined_Cleaned2,
        by = "MemberID",
        all.y = TRUE)

colnames(Compliance)[6] <- "Gap_Status"
#####
# write.csv(Compliance, "./data/output/Compliance.csv")

# # write.csv(Compliance, "C:/Users/RyanK/OneDrive - The NARBHA Institute/Documents - Data Force/Projects/AllianceIntranetSupport/data/Compliance.csv")
```

### Validation Matrix Results

The Validation Matrix dataset is a list of all distinct member IDs in the context of claims and VBP reports.

#### Validation

The cases when a member identified on the Health Choice VBP Quality Roster is also identified in adjudicated claims were as follows:

-   Valid ("Match"): `r Validation_Matrix |> filter(Match == "Match") |> nrow()` of the `r nrow(VBP_Unduplicated)` VBP members were validated against an eligible AMM Claims member. 
-   Not-Valid ("No Match"): `r ((nrow(VBP_Unduplicated)) - (Validation_Matrix |> filter(Match == "Match") |> nrow()))`
 were not matched.

#### Compliance

The compliance status of cases on the Health Choice VBP Report were as follows.

-   Non-Compliant: `r Compliance |> filter(Gap_Status == "OPEN") |> nrow()` 

-   Compliant: `r Compliance |> filter(Gap_Status == "CLOSED") |> nrow()` 

#### Validation by Compliance:

The matrix that results from combining the validation status and the compliance status of a given member provides the following results.

-   Matched, NonCompliant (MNC): `r Compliance |> filter(Gap_Status== "OPEN" & Match == "Match") |> nrow()` 

-   Matched, Compliant (MC): `r Compliance |> filter(Gap_Status== "CLOSED" & Match == "Match") |> nrow()`

-   NonMatched, NonCompliant (NMNC): `r Compliance |> filter(Gap_Status== "OPEN" & Match == "NoMatch") |> nrow()`

-   NonMatched, Compliant (NMC): `r Compliance |> filter(Gap_Status== "CLOSED" & Match == "NoMatch") |> nrow()`

## Analysis

The Pearson's Chi-Square test can be used to confirm if there is a significant difference between the expected result and the observed result in this comparison. 
Chi Square test was run at alpha = .05. There was not a statistical significance between the groups (*X*^2 (1, *N*=1691) = 30.64, *p*>.05), indicating that the groups in the Validation Matrix are not disproportionately impacted. Sampling error is consistent throughout the four groupings.

We also found that only 7% of the eligible cases found in adjudicated claims were identified by the VBP Quality Report.


```{r}
#| label: Chi Squared Test
#| eval: false
#| include: false
#| echo: true
#| warning: false
#| error: false


# transform for Chi Squared test
Comp_ChiSq <- Compliance |> 
    select(Match, Gap_Status) |> 
  group_by(Match, Gap_Status) |>
  #mutate(Numerator = as.character(Numerator)) |> 
  count() |> 
  pivot_wider(names_from = "Gap_Status",
              values_from = "n")
  # mutate(`0` = as.double(`0`),
  #        `1` = as.double(`1`))

# write.csv(Comp_ChiSq, "./data/output/Comp_ChiSq.csv")

chisq.test(Comp_ChiSq[-1], correct = FALSE)
```

```{r}
#| label: Chi Square Figure
#| eval: false
#| include: true
#| echo: true 
#| warning: false
#| error: false

library(ggplot2)

ChiSquare_Barchart <- Compliance |> 
  select(Match, Gap_Status) |> 
  group_by(Match, Gap_Status) |>
  mutate(Numerator = if_else(Gap_Status == "OPEN", "NonCompliant", "Compliant")) |> 
  count() |> 
  ggplot(aes(fill=Match, x=Gap_Status, y=n)) +
  geom_col() +
  scale_fill_manual(values = c(Match = "#b60ef9", NoMatch = "#520096"),
                    name = "Validation",
                    labels = c("Valid", "Not-Valid")) +
  labs(title = "VBP Report Member Event Validation",
       subtitle = "Distinct member validation against adjudicated claims",
       caption = "*From claims adjudicated through April 27, 2023") +
  ylab("Number of Distinct Members") +
  xlab("AMM2 Compliance Status") +
  scale_x_discrete(labels = c("Compliant", "Non-Compliant")) +
  theme(
    axis.title.y = element_text(vjust = 2),
    plot.subtitle = element_text(face = "italic"),
    plot.caption = element_text(face = "bold.italic", hjust = 0, vjust = -1)
  ) +
  annotate("text", x = 1, y = 125, label = "p < .001", color = "#b60ef9", fontface = "bold")

# Save the plot as a PNG file with a transparent background
ggsave("./data/output/chisquared_transparentbackground.png", width = 7, height = 5, dpi = 300, bg = "transparent")

# +
#   annotate("text", x = 1, y = 125, label = "p < .001", color = "#b60ef9", fontface = "bold")

# Save the plot as a PNG file
# ggsave("./data/output/AMMchisquare_barchart.png", width = 7, height = 5, dpi = 300)
# ggsave("C:/Users/RyanK/OneDrive - The NARBHA Institute/R_Studio/progressReport/images/20230630_AMMchisquare_barchart.png", width = 7, height = 5, dpi = 300)

ChiSquare_Barchart

```

# Discussion

## Eligibility Model

The process of determining eligible cases for the AMM measurement is very complex. We hope that we were able to adequately describe the process and the nature of the resulting dataset.

After determining the eligible cases, the process of validating that list of members against the VBP Quality Roster is fairly straight forward, and followed an almost identical process to the FUH7 Validation.

The evaluation of the aggregated Alliance Provider VBP Quality Rosters identified 1,691 eligible members. However, only 7% of those (131 members) could be validated with an eligible adjudicated claim. The question becomes, "Why are there so few matches?"

## Conclusions

Our goal is accuracy and transparency. With this in mind, The Alliance should keep a close eye on this measure. With the 1st quarter of data for the 2023 measurement year in the books, it is a concern that there are so few valid cases in the VBP Quality Report. This data will be reported to Health Choice leadership at our quarterly joint operating meeting

The current AMM performance score for the Alliance is 58.43%, or +1.14 percentage points above the mean. We recommend further investigation into provider tools and innovations to improve the overall performance of the network while we continue to assess data validation. 

# Quality Reviews

1.    Internal Alliance Leadership: 06-29-2023
2.    The Narbha Institute Leadership: 07-03-2023
3.    Internal Alliance Leadership: 07-06-2023
4.    Alliance-Health Choice Joint Operating Committee:

# Appendix

## Analysis output files

This project yields many tables of data along the way and places them in the "output" folder. The following is a table outlining the contents of that folder.

|#| File Name | Description | Variables | Observations |
|:-|:--|:-----|:-:|:-:|
|1.| PBMClaims | The raw data claims from the PBM database | 20 | 196,520 |
|2.| PBMClaims_NMHTest | PBMClaims + results from the IPSD and NMH tests | 22 | 196,520 |
|3.| MDDxClaims | The raw data claims from the claims database | 17 | 265,000 |
|4.| PBMClaims_MDDxTest | PBMClaims_NMHTest + results from the MDDx Test | 15 | 1,866,503 |
|5.| PBMClaims_CE_Test | PBMClaims_MDDxTest + results from the CE Test | 18 | 2,206,190 |
|6.| AMMElligibilityTest | PBMClaims_CE_Test + the final Elligibility test | 19 | 2,206,190 |
|7.| AMMEligibilityTest_AllEligible | AMMEligibilityTest filtered for only eligibible cases | 19 | 23,993 |
|8.| 2023-05-31_DataRaw_VBPQR_AllAPsCombined | The raw aggregated VBP QR data | 9 | 4,424 |
|9.| VBPQualityRoster | The aggregated and cleaned VBP QR data | 10 | 4,319 |
|10.| VBPQualityRoster_AMM | The VBPQualityRoster filtered for only AMM | 10 | 1,691 |
|11.| VBP_Validation | The VBPQualityRoster filtered to only include Member ID | 1 | 1,691 |
|12.| VBP_Unduplicated | An unduplicated list of members found on the VBP quality Roster for AMM | 2 | 1,691 |
|13.| AMMClaims_Unduplicated | An unduplicated list of members with eligible claims for the AMM memasure | 2 | 1,758 |
|14.| Validation Matrix | All unduplicated VBP and AMMClaims, matched on MemberID where possible | 4 | 3,326 |
|15.| Compliance | The validation Matrix, only including validated members, recombined with VBPQR data for compliance status | 13 | 1,691 |
|16.| Comp_ChiSq | A 2 x 2 matrix used for the Chi Square test | 3 | 2 |
|17.| My2023NDCtoGPICrosswalk | The NDC to GP Crosswalk | 5 | 1,104 |
|18.| AMM MemberFollowUpList | The list of people from the VALdation Martix, with thier matching indicators, recombined with PBM Claims data | 22 | 925,419 |

## All Alliance Performance Measures

![All Alliance Performance (Kivela, June 2023)](./images/2023-06-13_Tab_AllProvPerformance.png)[^9]

[^9]: Kivela, J.R. (June, 2023). Value Based Purchasing Report. *The Northern Arizona Regional Behavioral Health Alliance*.

## Create a GPI to NDC crosswalk

Because HCA categorizes their PBM claims in terms of Generic Product Identifier (GPI), but NCQA does theirs in terms of National Drug Code (NDC), we have to crosswalk the eligible AMM medications from NDC to their corresponding GPI codes. Fortunately, it's totally not a pain in the neck to backwards engineer this at all, lol.

::: panel-tabset
### Description {#sec-description}

The HEDIS My2023 Medication List Names identifies the eligible medications by providing the respective NDC numbers[^10]. The AHCCCS Preferred Drug List is used to match NDC numbers to GPI numbers based on medication name[^11].

[^10]: NCQA. (2023). *HEDIS Measures and Technical Resources*. https://www.ncqa.org/hedis/measures/

[^11]: Arizona Health Care Cost Containment System.*AHCCCS Drug List (Effective April 1, 2023)*. https://www.azahcccs.gov/Resources/Downloads/PharmacyUpdates/AHCCCS_DRUG_LIST_04012023.pdf

### Procedure {#sec-procedure}

In order to create a cross walk that ties the NDC codes from NCQA to the GPI codes from Health Choice, we took the following steps:

-   Import the My2023 Medication to Code data from the My2023 Medication List Directory[^12]
-   Filter the data to only include Antidepressant Medications
-   Rename some columns to provide consistency across tables
-   Import the AHCCCS Preferred Drug List [^13]. A side by side list of NDC and GPI codes is incredibly difficult to come by, and so we are using this list from 2019. Fortunately NDC and GPI are very stable.
-   Rename some columns to provide consistency across tables.
-   Filter Therapeutic Class for Antidepressant Other, Antidepressant SSRI
-   Merge the NCQA dataset with the AHCCCS dataset by matching on the the NDC variable.

[^12]: NCQA. (2023). *HEDIS Measures and Technical Resources*. https://www.ncqa.org/hedis/measures/

[^13]: Arizona Health Care Cost Containment System. *AHCCCS Drug List (Effective April 1, 2023)*. https://www.azahcccs.gov/Resources/Downloads/PharmacyUpdates/AHCCCS_DRUG_LIST_04012023.pdf

### Code {#sec-code}

```{r}
#| label: CreateRxIdentifierCrosswalk
#| eval: false
#| include: true
#| echo: true
#| code-fold: show
#| warning: false
#| error: false

# Covert NDC to GPI

#import the My2023 Medications to Code dataset
Med_To_NDC <- read.csv("./data/My2023MedicationToCode.csv")

Med_To_NDC_AMM <- Med_To_NDC |> 
  filter(Medication.List.Name == "Antidepressant Medications") |> 
  select(
    Medication.List.Name,
    Code,
    Generic.Product.Name
  )

colnames(Med_To_NDC_AMM)[colnames(Med_To_NDC_AMM) == "Code"] <- "NDC_NationalDrugCode"

colnames(Med_To_NDC_AMM)[colnames(Med_To_NDC_AMM) == "Medication.List.Name"] <- "MedicationList"

colnames(Med_To_NDC_AMM)[colnames(Med_To_NDC_AMM) == "Generic.Product.Name"] <- "GenericProductName"

# Import AHCCCS Preferred drug list
NDC_To_GPI <- read.csv("./data/AHCCCS_PreferredDrugListChangesFor_08012019.csv")

colnames(NDC_To_GPI)[colnames(NDC_To_GPI) == "Therapeutic.Class...Market.Basket"] <- "TherepeuticClass"

colnames(NDC_To_GPI)[colnames(NDC_To_GPI) == "National.Drug.Code..NDC..MediSpan"] <- "NDC_NationalDrugCode"

colnames(NDC_To_GPI)[colnames(NDC_To_GPI) == "MediSpan.Generic.Product.Indicator..GPI."] <- "GPI_GenericProductIdentifier"


NDC_To_GPI_AMM <- NDC_To_GPI |> 
  filter(c(TherepeuticClass == "ANTIDEPRESSANTS, OTHER" | TherepeuticClass ==  "ANTIDEPRESSANTS, SSRIs")) |> 
  select(NDC_NationalDrugCode,
         GPI_GenericProductIdentifier,
         TherepeuticClass)

# merge then filter for only AHCCCS preferred medications

# So these are all of the ones from NCQA that AHCCCS has on thier list. And now we have our crosswalk to address claims. 

My2023NDCtoGPICrosswalk <- merge(
  x = Med_To_NDC_AMM,
  y = NDC_To_GPI_AMM,
  by = "NDC_NationalDrugCode",
  all.y = TRUE
) |> 
  select(
    MedicationList,
    TherepeuticClass,
    NDC_NationalDrugCode,
    GPI_GenericProductIdentifier,
    GenericProductName
  ) |> 
  na.omit()

# write.csv(My2023NDCtoGPICrosswalk, "./data/output/My2023NDCtoGPICrosswalk.csv")

concatenated_values_GPI <- paste0("'",    My2023NDCtoGPICrosswalk$GPI_GenericProductIdentifier, "'", collapse = ", ")

```
:::

## Create an AMM Member Follow Up List

::: panel-tabset

### Description

The Validation Matrix and the original dataset for PBM Claims are combined to form a member list with an indicator for VBP, claims, validation, and the associated PBM claims variables.

### Procedure

The **Validation_Matrix** was transformed and rejoined with **AMMEligibilityTest** to create a table that will be used to generate Member Follow Up Lists. This new table is called **MemberFollowUpList**.

### Code

```{r}
#| label: AMM Member Follow Up List
#| eval: false
#| include: true
#| echo: true
#| code-fold: show
#| warning: false
#| error: false

# Using the copy of VBPQR_AllAPsCombined_Cleaned we made above, rename MemberID to match
# VBPQR_AllAPsCombined_Cleaned2 <- VBPQR_AllAPsCombined_Cleaned2 |>
#   rename("MemberID" = `Member ID`)

# Rejoin the matched and non-matched MemberIDs to their claims data
MemberFollowUpList <- 
   merge(x = Validation_Matrix,
        y = AMMEligibilityTest,
        by = "MemberID",
        all = TRUE) |>
  drop_na(Match)

# write.csv(MemberFollowUpList,"./data/output/AMM_MemberFollowUpList.csv")
# write.csv(MemberFollowUpList, "C:/Users/KGLtd/OneDrive - The NARBHA Institute/Documents - Data Force/Projects/AllianceIntranetSupport/data/output/AMM_MemberFollowUpList.csv")

```
:::

## Evaluate Validated Data

::: panel-tabset

### Description

Some exploratory analyses were conducted on the resulting data from this model. The results are included here for further reference, and we invite any innovations that spring from its review. 

### Top 10 Label Names

```{r}
#| label: Eval Label Name
#| eval: false
#| include: true
#| echo: true 
#| warning: false
#| error: false

MemberFollowUpList |> 
  filter(!is.na(LabelName)) |>
  group_by(LabelName) |> 
  summarise(percent = n() / nrow(MemberFollowUpList) * 100) |>
  arrange(desc(percent)) |>
  head(10) |>
  kable()

```

```{r}
#| label: Eval Label Name 2
#| eval: false
#| include: true
#| echo: true 
#| warning: false
#| error: false

MemberFollowUpList |>
  filter(!is.na(LabelName)) |>
  group_by(LabelName) |>
  summarise(percent = n() / nrow(MemberFollowUpList) * 100) |>
  filter(percent >= 1) |>
  arrange(LabelName) |>
  ggplot(aes(x = percent, y = reorder(LabelName, percent))) +
  geom_bar(stat = "identity", fill = "steelblue") +
  ylab(NULL) +
  xlab("Percentage of Total Prescription Fills") +
  ggtitle("Horizontal Bar Chart of Label Names with at least 1%") +
  theme_minimal() +
  theme(axis.text.y = element_text(angle = 0, hjust = 1, margin = margin(r = 5)),
        axis.title.y = element_blank(),
        plot.title = element_text(hjust = 1.0),
        axis.text.x = element_text(hjust = 0),
        axis.text.y.right = element_text(hjust = 0))
```

### Prescription Fill Trend

```{r}
#| label: Eval Date of Service
#| eval: false
#| include: true
#| echo: true 
#| warning: false
#| error: false

MemberFollowUpList |>
  mutate(dtefilled = as.Date(dtefilled)) |>
  mutate(month = floor_date(dtefilled, "month")) |>
  filter(month >= as.Date("2022-04-01") & month <= as.Date("2023-03-31")) |>
  count(month) |>
  ggplot(aes(x = month, y = n)) +
  geom_line() +
  geom_vline(xintercept = as.Date("2022-10-01"), linetype = "dashed", color = "#520096") +
  annotate("text", x = as.Date("2022-10-01"), y = max(MemberFollowUpList$n), label = "End RBHA Contract", 
           vjust = -10.0, hjust = -0.025, color = "#520096") +
  scale_x_date(date_labels = "%b %Y", date_breaks = "1 month") +
  scale_y_continuous(labels = comma) +
  ylab("Number of Fills") +
  xlab(NULL) +
  ggtitle("Frequency of Antidepressant Medication Fills over Time") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

:::
